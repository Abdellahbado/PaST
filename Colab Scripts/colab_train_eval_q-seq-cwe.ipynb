{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a971b497",
   "metadata": {},
   "source": [
    "# PaST â€” PPO (Colab/T4): CNN+DeepSets or CWE\n",
    "\n",
    "> If you want to run the **CWE** backbone (`cwe_sparse`), you must clone a repo/branch that includes the CWE changes.\n",
    "\n",
    "This notebook trains one PPO variant, then evaluates it with Greedy / SGBS / EAS or SGBS+EAS.\n",
    "\n",
    "**Typical flow**\n",
    "1) Install deps (Colab)\n",
    "2) Train: `python -m PaST.train_ppo ...`\n",
    "3) Auto-pick latest checkpoint\n",
    "4) Evaluate: `python -m PaST.run_eval_eas_* ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2760cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /content\n",
      "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "torch: 2.9.0+cu126\n",
      "cuda available: True\n",
      "gpu: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "print(\"CWD:\", ROOT)\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "# Make sure imports like `import PaST` work\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Basic sanity: torch + GPU\n",
    "import torch\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67929c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PaST'...\n",
      "remote: Enumerating objects: 356, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
      "remote: Total 356 (delta 10), reused 16 (delta 5), pack-reused 329 (from 1)\u001b[K\n",
      "Receiving objects: 100% (356/356), 125.83 MiB | 39.72 MiB/s, done.\n",
      "Resolving deltas: 100% (178/178), done.\n"
     ]
    }
   ],
   "source": [
    "# Colab installs (safe to re-run).\n",
    "# If you're not on Colab and already have deps, you can skip this cell.\n",
    "#\n",
    "# IMPORTANT: The default upstream repo may not include the CWE variant yet.\n",
    "# If you want to run CWE, point these to the branch where you added it.\n",
    "REPO_URL = \"https://github.com/Abdellahbado/PaST\"  # TODO: change to your fork if needed\n",
    "REPO_BRANCH = \"main\"  # TODO: change to your CWE branch if needed\n",
    "\n",
    "!rm -rf PaST\n",
    "!git clone -b \"$REPO_BRANCH\" \"$REPO_URL\"\n",
    "\n",
    "!pip -q install -r PaST/requirements.txt\n",
    "!pip -q install pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a9156",
   "metadata": {},
   "source": [
    "## Train (long-run-friendly hyperparams)\n",
    "\n",
    "Defaults here are chosen to **keep learning for a long time**:\n",
    "- Cosine LR decay with a non-trivial end LR (`lr_end_factor=0.1`)\n",
    "- Cosine entropy decay (explore early, still some exploration late)\n",
    "- Target KL to prevent updates collapsing\n",
    "- Curriculum enabled (helps early stability): starts on *small* horizons then gradually introduces larger ones, while also annealing the epsilon-constraint slack range\n",
    "\n",
    "Adjust `TOTAL_ENV_STEPS` depending on runtime budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory in Drive\n",
    "import os\n",
    "drive_checkpoint_dir = '/content/drive/MyDrive/PaST_checkpoints'\n",
    "os.makedirs(drive_checkpoint_dir, exist_ok=True)\n",
    "print(f\"Checkpoint directory: {drive_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75870e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3 -m PaST.train_q_sequence --variant_id q_sequence_cwe_ctx13 --seed 0 --device cuda --output_dir runs_colab --num_rounds 50 --episodes_per_round 2048 --warmup_rounds 5 --collection_batch_size 128 --batch_size 128 --num_dataloader_workers 2 --num_collection_workers 1 --num_epochs_per_round 4 --learning_rate 0.0001 --weight_decay 1e-05 --completion_policy mix --completion_prob_start 1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "# Pick ONE variant_id:\n",
    "# - q_sequence_cwe_ctx13 (CWE + CTX13, T4 optimized)\n",
    "#\n",
    "VARIANT_ID = \"q_sequence_cwe_ctx13\"\n",
    "SEED = 0\n",
    "OUT_DIR = \"/content/drive/MyDrive/PaST_checkpoints/runs_q_sequence\"\n",
    "\n",
    "# Main training knobs (T4-friendly Q-Learning)\n",
    "NUM_ROUNDS = 50\n",
    "EPISODES_PER_ROUND = 2048   # High throughput collection on T4 (16 batches of 128)\n",
    "COLLECTION_BATCH_SIZE = 128 # Maximize GPU vectorization (Parallel envs)\n",
    "TRAIN_BATCH_SIZE = 128      # Train batch size\n",
    "EPOCHS_PER_ROUND = 4        # Supervised epochs per DAgger round\n",
    "\n",
    "LR = 1e-4\n",
    "WD = 1e-5\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    \"-m\",\n",
    "    \"PaST.train_q_sequence\",\n",
    "    \"--variant_id\",\n",
    "    VARIANT_ID,\n",
    "    \"--seed\",\n",
    "    str(SEED),\n",
    "    \"--device\",\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"--output_dir\",\n",
    "    OUT_DIR,\n",
    "    # DAgger Loop\n",
    "    \"--num_rounds\",\n",
    "    str(NUM_ROUNDS),\n",
    "    \"--episodes_per_round\",\n",
    "    str(EPISODES_PER_ROUND),\n",
    "    \"--warmup_rounds\",\n",
    "    \"5\",\n",
    "    \n",
    "    # Batching & Parallelization\n",
    "    \"--collection_batch_size\",\n",
    "    str(COLLECTION_BATCH_SIZE),\n",
    "    \"--batch_size\",\n",
    "    str(TRAIN_BATCH_SIZE),\n",
    "    \"--num_dataloader_workers\",\n",
    "    \"2\",  # 2 workers pre-fetch data for the GPU trainer\n",
    "    \"--num_collection_workers\",\n",
    "    \"1\",  # 1 is optimal for GPUBatchSequenceEnv (Vectorized on GPU > Multiprocessing)\n",
    "    \n",
    "    # Optimization\n",
    "    \"--num_epochs_per_round\",\n",
    "    str(EPOCHS_PER_ROUND),\n",
    "    \"--learning_rate\",\n",
    "    str(LR),\n",
    "    \"--weight_decay\",\n",
    "    str(WD),\n",
    "    \n",
    "    # Policy Strategy\n",
    "    \"--completion_policy\",\n",
    "    \"mix\",\n",
    "    \"--completion_prob_start\",\n",
    "    \"1.0\",\n",
    "]\n",
    "\n",
    "print(\" \".join(cmd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b48e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CPU setup: os.cpu_count=2 | torch_threads=2 | collection_workers=1 | dataloader_workers=2\n",
      "Model parameters: 466,562\n",
      "\n",
      "============================================================\n",
      "Q-Sequence Training: q_sequence_cwe_ctx13\n",
      "Output: runs_colab/q_sequence_cwe_ctx13_s0_20260122_153027\n",
      "Warmup rounds (SPT completion): 5\n",
      "Completion policy: mix | mix_prob_start=1.0 | mix_prob_end=1.0\n",
      "============================================================\n",
      "\n",
      "Round 1/50 [SPT]: Collecting... 100071 transitions in 307.3s\n",
      "  Training (100000 in buffer)... loss=549.9055, mae=550.40, listwise=0.0000 in 99.7s\n",
      "Round 2/50 [SPT]: Collecting... 100847 transitions in 321.5s\n",
      "  Training (100000 in buffer)... loss=359.1475, mae=359.65, listwise=0.0000 in 100.8s\n",
      "Round 3/50 [SPT]: Collecting... 100072 transitions in 324.6s\n",
      "  Training (100000 in buffer)... loss=353.6341, mae=354.13, listwise=0.0000 in 100.9s\n",
      "Round 4/50 [SPT]: Collecting... 100179 transitions in 326.5s\n",
      "  Training (100000 in buffer)... loss=268.9048, mae=269.40, listwise=0.0000 in 101.1s\n",
      "Round 5/50 [SPT]: Collecting... 102595 transitions in 325.5s\n",
      "  Training (100000 in buffer)... loss=209.1140, mae=209.61, listwise=0.0000 in 99.4s\n",
      "  Evaluating... cost=400.12, vs_spt=0.1%\n",
      "Round 6/50 [model]: Collecting... 95302 transitions in 1177.8s\n",
      "  Training (100000 in buffer)... loss=201.7974, mae=202.30, listwise=0.0000 in 102.2s\n",
      "Round 7/50 [model]: Collecting... 100457 transitions in 1281.2s\n",
      "  Training (100000 in buffer)... loss=171.3615, mae=171.86, listwise=0.0000 in 102.0s\n",
      "Round 8/50 [model]: Collecting... 102082 transitions in 1315.3s\n",
      "  Training (100000 in buffer)... loss=141.7971, mae=142.30, listwise=0.0000 in 99.3s\n",
      "Round 9/50 [model]: Collecting... 98721 transitions in 1260.4s\n",
      "  Training (100000 in buffer)... loss=172.7761, mae=173.27, listwise=0.0000 in 101.8s\n",
      "Round 10/50 [model]: Collecting... 94969 transitions in 1206.4s\n",
      "  Training (100000 in buffer)... loss=134.0217, mae=134.52, listwise=0.0000 in 99.9s\n",
      "  Evaluating... cost=401.29, vs_spt=-0.2%\n",
      "Round 11/50 [model]: Collecting... 93085 transitions in 1192.3s\n",
      "  Training (100000 in buffer)... loss=133.0391, mae=133.54, listwise=0.0000 in 98.3s\n",
      "Round 12/50 [model]: Collecting... 99289 transitions in 1278.5s\n",
      "  Training (100000 in buffer)... loss=146.8387, mae=147.34, listwise=0.0000 in 101.5s\n",
      "Round 13/50 [model]: Collecting... 97233 transitions in 1240.2s\n",
      "  Training (100000 in buffer)... loss=133.4162, mae=133.91, listwise=0.0000 in 101.3s\n",
      "Round 14/50 [model]: Collecting... 99921 transitions in 1290.1s\n",
      "  Training (100000 in buffer)... loss=110.2470, mae=110.75, listwise=0.0000 in 98.0s\n",
      "Round 15/50 [model]: Collecting... "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://8080-gpu-t4-s-coz9g1rzzz0k-c.europe-west4-0.prod.colab.dev/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "!{' '.j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7f1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d4041bc",
   "metadata": {},
   "source": [
    "## Find the latest checkpoint\n",
    "This grabs the most recent `latest.pt` under the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0b3e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No latest.pt found under runs_colab/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1800429877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mckpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{OUT_DIR}/**/checkpoints/latest.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mckpts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No latest.pt found under {OUT_DIR}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mckpts_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No latest.pt found under runs_colab/"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "ckpts = glob.glob(f\"{OUT_DIR}/**/checkpoints/latest.pt\", recursive=True)\n",
    "if not ckpts:\n",
    "    raise FileNotFoundError(f\"No latest.pt found under {OUT_DIR}/\")\n",
    "\n",
    "ckpts_sorted = sorted(ckpts, key=lambda p: Path(p).stat().st_mtime)\n",
    "CKPT = ckpts_sorted[-1]\n",
    "print(\"Using checkpoint:\", CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed3550",
   "metadata": {},
   "source": [
    "## Evaluate (Greedy / SGBS / EAS / SGBS+EAS)\n",
    "Choose the script based on the variant family:\n",
    "- `run_eval_eas_family_q4_beststart` for `ppo_family_*_beststart_*`\n",
    "- `run_eval_eas_duration_aware` for `ppo_duration_aware_*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f594c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_SEED = 42\n",
    "NUM_INSTANCES = 16\n",
    "SCALE = \"small\"  # small|medium|large\n",
    "EPS_STEPS = 5\n",
    "\n",
    "METHOD = \"sgbs_eas\"  # eas|sgbs_eas\n",
    "BETA = \"4\"\n",
    "GAMMA = \"4\"\n",
    "MAX_ITERS = 50\n",
    "EAS_LR = 0.003\n",
    "EAS_IL = 0.01\n",
    "SAMPLES_PER_ITER = 32\n",
    "\n",
    "if \"duration_aware\" in VARIANT_ID:\n",
    "    eval_mod = \"PaST.run_eval_eas_duration_aware\"\n",
    "else:\n",
    "    eval_mod = \"PaST.run_eval_eas_family_q4_beststart\"\n",
    "\n",
    "eval_cmd = [\n",
    "    sys.executable,\n",
    "    \"-m\",\n",
    "    eval_mod,\n",
    "    \"--checkpoint\",\n",
    "    CKPT,\n",
    "    \"--variant_id\",\n",
    "    VARIANT_ID,\n",
    "    \"--eval_seed\",\n",
    "    str(EVAL_SEED),\n",
    "    \"--num_instances\",\n",
    "    str(NUM_INSTANCES),\n",
    "    \"--scale\",\n",
    "    SCALE,\n",
    "    \"--epsilon_steps\",\n",
    "    str(EPS_STEPS),\n",
    "    \"--method\",\n",
    "    METHOD,\n",
    "    \"--beta\",\n",
    "    BETA,\n",
    "    \"--gamma\",\n",
    "    GAMMA,\n",
    "    \"--max_iterations\",\n",
    "    str(MAX_ITERS),\n",
    "    \"--eas_lr\",\n",
    "    str(EAS_LR),\n",
    "    \"--eas_il_weight\",\n",
    "    str(EAS_IL),\n",
    "    \"--samples_per_iter\",\n",
    "    str(SAMPLES_PER_ITER),\n",
    "]\n",
    "print(\" \".join(eval_cmd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{' '.join(eval_cmd)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b3a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
